{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data base import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_data\n",
    "\n",
    "# -- Get the data -- #\n",
    "NB_SAMPLES = 10\n",
    "PATH_SAMPLE = Path(\"../data/samples/sample_{}_train.csv\".format(NB_SAMPLES))\n",
    "DATA_SAMPLE = pd.read_csv(PATH_SAMPLE).to_numpy()\n",
    "\n",
    "\n",
    "# -- Clean the data -- #\n",
    "from utils.clean_data import clean_data\n",
    "DATA_SAMPLE = clean_data(DATA_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20951ebfde'\n",
      "  ' While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!'\n",
      "  'While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it'\n",
      "  0]\n",
      " ['e283379cad'\n",
      "  'hates the net. ayaw bumukas ng twitter.  http://plurk.com/p/wxlxs'\n",
      "  'hates the net.' -1]\n",
      " ['0bc3d2ce9f'\n",
      "  'Im feeling so nostalgic. Im sad. But happy. I don`t now how to feel. It`s over, but not at the same time. It just feels over.  i love you.'\n",
      "  'love' 1]\n",
      " ['e316d2f54b' '_in_NH night bud' '_in_NH night bud' 0]\n",
      " ['57d554e002' 'time for work!' 'time for work!' 0]\n",
      " ['35d95a6bfa'\n",
      "  'http://twitpic.com/4wqfv - dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  'dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  1]\n",
      " ['a2f6c5cca9'\n",
      "  'Playing with the munchkin today, talking cakes and getting ready for a yard sale tomorrow. Not looking forward to that'\n",
      "  'Not looking forward to that' -1]\n",
      " ['dcacc9ef8f'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  0]\n",
      " ['e668df7ceb' '  I hope you have a nice sleep' 'a nice' 1]]\n"
     ]
    }
   ],
   "source": [
    "print(DATA_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "#### TWEET_ORIGINAL : List of the tweets : Array, shape = (len(nb_tweets))\n",
    "#### TWEET_STRING : List of the list of the word of each tweet : List of list of string\n",
    "#### TWEET_SCALAR : List of the description of each tweet : Array, shape = (len(nb_tweets), sentence_size * word_size)\n",
    "#### IMPORTANT_WORDS : List of the label of each tweet : Array, shape = (len(nb_tweets), sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Parameters -- #\n",
    "WORD_SIZE = 50  # 50 or 100 or 200 or 300\n",
    "SENTENCE_SIZE = 50  # What ever\n",
    "FILL_WITH = 0  # If a word is not in the dictionary, [0, ..., 0] will describe it.\n",
    "OPTIONS = [WORD_SIZE, SENTENCE_SIZE, FILL_WITH]\n",
    "\n",
    "SENTIMENT_WEIGHT = 2  # Multiply the sentiment by a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet :\n",
      " While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!\n",
      "Shape of TWEET_ORIGINAL : (9,)\n"
     ]
    }
   ],
   "source": [
    "# -- Get the original tweets -- #\n",
    "TWEET_ORIGINAL = DATA_SAMPLE[:, 1]\n",
    "print(\"First tweet :\")\n",
    "print(TWEET_ORIGINAL[0])\n",
    "print(\"Shape of TWEET_ORIGINAL :\", TWEET_ORIGINAL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition of the first tweet :\n",
      "['While', 'im', 'stuck', 'INSIDE', 'in', 'Elk', 'Grove', 'Village', 'working', 'all', 'day', 'Someone', 'should', 'enjoy', 'it', '!']\n",
      "Length of TWEET_STRING : 9\n"
     ]
    }
   ],
   "source": [
    "from descriptors.tweet_string.create_strings import create_strings\n",
    "from descriptors.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "TOKENIZER = Tokenizer()\n",
    "\n",
    "# -- Get the decomposition of the tweets -- #\n",
    "TWEET_STRING = create_strings(TWEET_ORIGINAL, TOKENIZER)\n",
    "print(\"Decomposition of the first tweet :\")\n",
    "print(TWEET_STRING[0])\n",
    "print(\"Length of TWEET_STRING :\", len(TWEET_STRING))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.descriptor_glove.descriptor_glove import tweet_scalar_glove\n",
    "from utils.standardize import standardize\n",
    "\n",
    "# Get the dictionary\n",
    "PATH_DICTIONARY = Path(\"../data/glove_descriptor/glove.6B.{}d.txt\".format(WORD_SIZE))\n",
    "DICTIONARY = pd.read_csv(PATH_DICTIONARY, sep=\" \", header=None)\n",
    "\n",
    "# Additional dictionary\n",
    "ADDITIONAL_DIC = {\"..\": \"...\", \"<3\": \"love\"}\n",
    "\n",
    "# Get the sentiments\n",
    "SENTIMENTS = DATA_SAMPLE[:, -1]\n",
    "\n",
    "# -- Get the decriptions of each tweets -- #\n",
    "TWEET_SCALAR = tweet_scalar_glove(TWEET_STRING, SENTIMENTS, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "\n",
    "# Standardize the tweet descriptions\n",
    "standardize(TWEET_SCALAR)\n",
    "\n",
    "print(\"Descroption of the first tweet :\")\n",
    "print(TWEET_SCALAR[0])\n",
    "print(\"Shape of TWEET_SCLALAR :\", TWEET_SCALAR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descritpors.tweet_label.create_labels import create_labels\n",
    "\n",
    "# Create the decompositions of the labels\n",
    "LABEL_ORIGINAL = DATA_SAMPLE[:, 1]\n",
    "LABEL_STRING = create_strings(LABEL_ORIGINAL, TOKENIZER, OPTIONS)\n",
    "\n",
    "# -- Get the labels -- #\n",
    "IMPORTANT_WORDS = create_labels(TWEET_STRING, LABEL_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neighbors = 1\n",
    "\n",
    "knn = KNeighborsRegressor(nb_neighbors, weights=\"distance\")\n",
    "knn.fit(TWEET_SCALAR, IMPORTANT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = knn.predict(TWEET_SCALAR)\n",
    "\n",
    "print(TWEET_ORIGINAL)\n",
    "print(TWEET_ORIGINAL.shape)\n",
    "print(np.array(TWEET_STRING))\n",
    "print(TWEET_STRING.shape)\n",
    "print(PREDICTIONS)\n",
    "print(PREDICTIONS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.post_processing import pred_to_string\n",
    "\n",
    "RESULTS = pred_to_string(TWEET_ORIGINAL, TWEET_STRING, PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(DATA[i, 1])\n",
    "    print(DATA[i, 2])\n",
    "    print(RESULTS[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss import jaccard\n",
    "\n",
    "AVG = 0\n",
    "for i in range(len(RESULTS)):\n",
    "    AVG += jaccard(RESULTS[i], DATA[i, 2])\n",
    "    \n",
    "    if jaccard(RESULTS[i], DATA_SAMPLE[i, 2]) != 1:\n",
    "        print(\"Error on\", i)\n",
    "AVG /= len(RESULTS)\n",
    "\n",
    "print(AVG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
