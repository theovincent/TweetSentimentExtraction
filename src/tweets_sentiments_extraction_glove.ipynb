{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data base import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_data\n",
    "\n",
    "# -- Get the data -- #\n",
    "NB_SAMPLES = 1000\n",
    "TRAIN_SAMPLE = Path(\"../data/samples/sample_{}_train.csv\".format(NB_SAMPLES))\n",
    "VALID_SAMPLE = Path(\"../data/samples/sample_{}_validation.csv\".format(NB_SAMPLES))\n",
    "TRAIN_SAMPLE = pd.read_csv(TRAIN_SAMPLE).to_numpy()\n",
    "VALID_SAMPLE = pd.read_csv(VALID_SAMPLE).to_numpy()\n",
    "\n",
    "\n",
    "# -- Clean the data -- #\n",
    "from utils.clean_data import clean_data\n",
    "TRAIN_SAMPLE = clean_data(TRAIN_SAMPLE)\n",
    "VALID_SAMPLE = clean_data(VALID_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "#### TWEET_ORIGINALS : List of the tweets : Array, shape = (len(nb_tweets))\n",
    "#### TWEET_STRINGS : List of the list of the word of each tweet : List of list of string\n",
    "#### TWEET_SCALARS : List of the description of each tweet : Array, shape = (len(nb_tweets), sentence_size * word_size)\n",
    "#### IMPORTANT_WORDS : List of the label of each tweet : Array, shape = (len(nb_tweets), sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Parameters -- #\n",
    "WORD_SIZE = 50  # 50 or 100 or 200 or 300\n",
    "FILL_WITH = 0  # If a word is not in the dictionary, [0, ..., 0] will describe it.\n",
    "SENTIMENT_WEIGHT = 2  # Multiply the sentiment by a factor\n",
    "SENTENCE_SIZE = 50  # What ever\n",
    "OPTIONS = [WORD_SIZE, SENTENCE_SIZE, FILL_WITH, SENTIMENT_WEIGHT]\n",
    "\n",
    "\n",
    "# -- Get the original tweets -- #\n",
    "TWEET_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "TWEET_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "print(\"First tweet :\")\n",
    "print(TWEET_ORIGINALS_TRAIN[0])\n",
    "print(\"Shape of TWEET_ORIGINAL :\", TWEET_ORIGINALS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.tweet_string.create_strings import create_strings\n",
    "from descriptors.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "TOKENIZER = Tokenizer()\n",
    "\n",
    "# -- Get the decomposition of the tweets -- #\n",
    "TWEET_STRINGS_TRAIN = create_strings(TWEET_ORIGINALS_TRAIN, TOKENIZER, SENTENCE_SIZE)\n",
    "TWEET_STRINGS_VALID = create_strings(TWEET_ORIGINALS_VALID, TOKENIZER, SENTENCE_SIZE)\n",
    "print(\"Decomposition of the first tweet :\")\n",
    "print(TWEET_STRINGS_TRAIN[0])\n",
    "print(\"Length of TWEET_STRING :\", len(TWEET_STRINGS_TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.descriptor_glove.descriptor_glove import tweet_scalar_glove\n",
    "from utils.standardize import standardize\n",
    "\n",
    "\n",
    "# Get the dictionary\n",
    "PATH_DICTIONARY = Path(\"../data/glove_descriptor/glove.6B.{}d.txt\".format(WORD_SIZE))\n",
    "# PATH_DICTIONARY = Path(\"../data/glove_descriptor/sample_test.txt\")\n",
    "DICTIONARY = pd.read_csv(PATH_DICTIONARY, sep=\" \", header=None)\n",
    "\n",
    "# Additional dictionary\n",
    "ADDITIONAL_DIC = {\"..\": \"...\", \"<3\": \"love\"}\n",
    "\n",
    "# Get the sentiments\n",
    "SENTIMENTS_TRAIN = TRAIN_SAMPLE[:, -1]\n",
    "SENTIMENTS_VALID = VALID_SAMPLE[:, -1]\n",
    "\n",
    "# -- Get the decriptions of each tweets -- #\n",
    "TWEET_SCALARS_TRAIN = tweet_scalar_glove(TWEET_STRINGS_TRAIN, SENTIMENTS_TRAIN, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "TWEET_SCALARS_VALID = tweet_scalar_glove(TWEET_STRINGS_VALID, SENTIMENTS_VALID, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "\n",
    "# Standardize the tweet descriptions\n",
    "standardize(TWEET_SCALARS_TRAIN)\n",
    "standardize(TWEET_SCALARS_VALID)\n",
    "\n",
    "print(\"Description of the first tweet :\")\n",
    "print(TWEET_SCALARS_TRAIN[0])\n",
    "print(\"Shape of TWEET_SCLALAR :\", TWEET_SCALARS_TRAIN.shape)\n",
    "print(TWEET_SCALARS_VALID.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.tweet_label.create_labels import create_labels\n",
    "\n",
    "# Create the decompositions of the labels\n",
    "LABEL_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 2]\n",
    "LABEL_ORIGINALS_VALID = VALID_SAMPLE[:, 2]\n",
    "LABEL_STRINGS_TRAIN = create_strings(LABEL_ORIGINALS_TRAIN, TOKENIZER, SENTENCE_SIZE)\n",
    "LABEL_STRINGS_VALID = create_strings(LABEL_ORIGINALS_VALID, TOKENIZER, SENTENCE_SIZE)\n",
    "\n",
    "# -- Get the labels -- #\n",
    "IMPORTANT_WORDS_TRAIN = create_labels(TWEET_STRINGS_TRAIN, LABEL_STRINGS_TRAIN, SENTENCE_SIZE)\n",
    "IMPORTANT_WORDS_VALID = create_labels(TWEET_STRINGS_VALID, LABEL_STRINGS_VALID, SENTENCE_SIZE)\n",
    "\n",
    "IDX = 5\n",
    "print(TWEET_ORIGINALS_TRAIN[IDX])\n",
    "print(LABEL_ORIGINALS_TRAIN[IDX])\n",
    "print(\"Labels :\")\n",
    "print(IMPORTANT_WORDS_TRAIN[IDX])\n",
    "print(\"Shape of IMPORTANT_WORDS :\", IMPORTANT_WORDS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with one KNN\n",
    "NB_NEIGHBORS = 1\n",
    "\n",
    "KNN = KNeighborsRegressor(NB_NEIGHBORS, weights=\"distance\")\n",
    "KNN.fit(TWEET_SCALARS_TRAIN, IMPORTANT_WORDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimize the number of neighbors --- #\n",
    "# Parameter\n",
    "NB_NEIGHBORS_MAX = 200\n",
    "NB_NEIGHBORS_MIN = 80\n",
    "STEP = 10\n",
    "\n",
    "# Variables\n",
    "NB_NEIGHBORS_OPT = 1\n",
    "JACCARD_ACC_MAX = 0\n",
    "JACCARD_LIST = []\n",
    "\n",
    "for nb_neigh in range(NB_NEIGHBORS_MIN, NB_NEIGHBORS_MAX, STEP):\n",
    "    # Define the knn\n",
    "    KNN = KNeighborsRegressor(nb_neigh, weights=\"distance\")\n",
    "    \n",
    "    # Train the knn\n",
    "    KNN.fit(TWEET_SCALARS_TRAIN, IMPORTANT_WORDS_TRAIN)\n",
    "    \n",
    "    # Scalar predictions\n",
    "    PRED_SCALAR = KNN.predict(TWEET_SCALARS_VALID)\n",
    "    \n",
    "    # Get the string predictions\n",
    "    PRED_STRING = preds_to_strings(TWEET_ORIGINALS_VALID, TWEET_STRINGS_VALID, PRED_SCALAR)\n",
    "    \n",
    "    # Compute the loss\n",
    "    JACCARD_ACC = mean_jaccard(LABEL_ORIGINALS_VALID, PRED_STRING)\n",
    "    \n",
    "    # Updates\n",
    "    JACCARD_LIST.append(JACCARD_ACC)\n",
    "    if JACCARD_ACC > JACCARD_ACC_MAX:\n",
    "        JACCARD_ACC_MAX = JACCARD_ACC\n",
    "        NB_NEIGHBORS_OPT = nb_neigh    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the results\n",
    "PATH_SAVE = Path(\"../results\")\n",
    "\n",
    "print(\"The optimal number of neighbors to take is :\", NB_NEIGHBORS_OPT)\n",
    "print(\"With this number, the Jaccard accuracy is :\", JACCARD_ACC_MAX)\n",
    "\n",
    "plt.plot(np.arange(NB_NEIGHBORS_MIN, NB_NEIGHBORS_MAX, STEP), JACCARD_LIST, label=\"Jaccard score\")\n",
    "plt.xlabel(\"nb neighbors\")\n",
    "plt.legend()\n",
    "plt.savefig(PATH_SAVE / \"glove_knn_train_{}_neighbors_{}.jpg\".format(NB_SAMPLES, NB_NEIGHBORS_MAX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SCALAR = KNN.predict(TWEET_SCALARS_VALID)\n",
    "print(\"Nomber of correct match\", np.sum(PREDICTIONS == IMPORTANT_WORDS_VALID))\n",
    "print(\"Number of match to make\", len(PREDICTIONS) * len(PREDICTIONS[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.post_processing import preds_to_strings\n",
    "\n",
    "PRED_STRING = preds_to_strings(TWEET_ORIGINALS_VALID, TWEET_STRINGS_VALID, PRED_SCALAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_RESULT = False\n",
    "if SHOW_RESULT:\n",
    "    for idx_tweet in range(len(RESULTS)):\n",
    "        print(TWEET_ORIGINALS_VALID[idx_tweet])\n",
    "        print(IMPORTANT_WORDS_VALID[idx_tweet])\n",
    "        print(PRED_SCALAR[idx_tweet])\n",
    "        print(PRED_STRING[idx_tweet], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss import mean_jaccard\n",
    "\n",
    "JACCARD_ACC = mean_jaccard(LABEL_ORIGINALS_VALID, PRED_STRING)\n",
    "print(JACCARD_ACC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
