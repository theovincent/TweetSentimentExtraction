{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data base import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_data\n",
    "\n",
    "# -- Get the data -- #\n",
    "NB_SAMPLES = 10\n",
    "TRAIN_SAMPLE = Path(\"../data/samples/sample_{}_train.csv\".format(NB_SAMPLES))\n",
    "VALID_SAMPLE = Path(\"../data/samples/sample_{}_validation.csv\".format(NB_SAMPLES))\n",
    "TRAIN_SAMPLE = pd.read_csv(TRAIN_SAMPLE).to_numpy()\n",
    "VALID_SAMPLE = pd.read_csv(VALID_SAMPLE).to_numpy()\n",
    "\n",
    "\n",
    "# -- Clean the data -- #\n",
    "from utils.clean_data import clean_data\n",
    "TRAIN_SAMPLE = clean_data(TRAIN_SAMPLE)\n",
    "VALID_SAMPLE = clean_data(VALID_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20951ebfde'\n",
      "  ' While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!'\n",
      "  'While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it'\n",
      "  0]\n",
      " ['e283379cad'\n",
      "  'hates the net. ayaw bumukas ng twitter.  http://plurk.com/p/wxlxs'\n",
      "  'hates the net.' -1]\n",
      " ['0bc3d2ce9f'\n",
      "  'Im feeling so nostalgic. Im sad. But happy. I don`t now how to feel. It`s over, but not at the same time. It just feels over.  i love you.'\n",
      "  'love' 1]\n",
      " ['e316d2f54b' '_in_NH night bud' '_in_NH night bud' 0]\n",
      " ['57d554e002' 'time for work!' 'time for work!' 0]\n",
      " ['35d95a6bfa'\n",
      "  'http://twitpic.com/4wqfv - dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  'dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  1]\n",
      " ['a2f6c5cca9'\n",
      "  'Playing with the munchkin today, talking cakes and getting ready for a yard sale tomorrow. Not looking forward to that'\n",
      "  'Not looking forward to that' -1]\n",
      " ['dcacc9ef8f'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  0]\n",
      " ['e668df7ceb' '  I hope you have a nice sleep' 'a nice' 1]]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "#### TWEET_ORIGINALS : List of the tweets : Array, shape = (len(nb_tweets))\n",
    "#### TWEET_STRINGS : List of the list of the word of each tweet : List of list of string\n",
    "#### TWEET_SCALARS : List of the description of each tweet : Array, shape = (len(nb_tweets), sentence_size * word_size)\n",
    "#### IMPORTANT_WORDS : List of the label of each tweet : Array, shape = (len(nb_tweets), sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet :\n",
      " While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!\n",
      "Shape of TWEET_ORIGINAL : (9,)\n"
     ]
    }
   ],
   "source": [
    "# -- Get the original tweets -- #\n",
    "TWEET_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "TWEET_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "print(\"First tweet :\")\n",
    "print(TWEET_ORIGINALS_TRAIN[0])\n",
    "print(\"Shape of TWEET_ORIGINAL :\", TWEET_ORIGINALS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition of the first tweet :\n",
      "['While', 'im', 'stuck', 'INSIDE', 'in', 'Elk', 'Grove', 'Village', 'working', 'all', 'day', 'Someone', 'should', 'enjoy', 'it', '!']\n",
      "Length of TWEET_STRING : 9\n"
     ]
    }
   ],
   "source": [
    "from descriptors.tweet_string.create_strings import create_strings\n",
    "from descriptors.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "TOKENIZER = Tokenizer()\n",
    "\n",
    "# -- Get the decomposition of the tweets -- #\n",
    "TWEET_STRINGS_TRAIN = create_strings(TWEET_ORIGINALS_TRAIN, TOKENIZER)\n",
    "TWEET_STRINGS_VALID = create_strings(TWEET_ORIGINALS_VALID, TOKENIZER)\n",
    "print(\"Decomposition of the first tweet :\")\n",
    "print(TWEET_STRINGS_TRAIN[0])\n",
    "print(\"Length of TWEET_STRING :\", len(TWEET_STRINGS_TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the first tweet :\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Shape of TWEET_SCLALAR : (9, 2501)\n"
     ]
    }
   ],
   "source": [
    "from descriptors.descriptor_glove.descriptor_glove import tweet_scalar_glove\n",
    "from utils.standardize import standardize\n",
    "\n",
    "# -- Parameters -- #\n",
    "WORD_SIZE = 50  # 50 or 100 or 200 or 300\n",
    "SENTENCE_SIZE = 50  # What ever\n",
    "FILL_WITH = 0  # If a word is not in the dictionary, [0, ..., 0] will describe it.\n",
    "SENTIMENT_WEIGHT = 2  # Multiply the sentiment by a factor\n",
    "OPTIONS = [WORD_SIZE, SENTENCE_SIZE, FILL_WITH, SENTIMENT_WEIGHT]\n",
    "\n",
    "# Get the dictionary\n",
    "# PATH_DICTIONARY = Path(\"../data/glove_descriptor/glove.6B.{}d.txt\".format(WORD_SIZE))\n",
    "PATH_DICTIONARY = Path(\"../data/glove_descriptor/sample_test.txt\")\n",
    "DICTIONARY = pd.read_csv(PATH_DICTIONARY, sep=\" \", header=None)\n",
    "\n",
    "# Additional dictionary\n",
    "ADDITIONAL_DIC = {\"..\": \"...\", \"<3\": \"love\"}\n",
    "\n",
    "# Get the sentiments\n",
    "SENTIMENTS_TRAIN = TRAIN_SAMPLE[:, -1]\n",
    "SENTIMENTS_VALID = VALID_SAMPLE[:, -1]\n",
    "\n",
    "# -- Get the decriptions of each tweets -- #\n",
    "TWEET_SCALARS_TRAIN = tweet_scalar_glove(TWEET_STRINGS_TRAIN, SENTIMENTS_TRAIN, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "TWEET_SCALARS_VALID = tweet_scalar_glove(TWEET_STRINGS_VALID, SENTIMENTS_VALID, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "\n",
    "# Standardize the tweet descriptions\n",
    "standardize(TWEET_SCALARS_TRAIN)\n",
    "standardize(TWEET_SCALARS_VALID)\n",
    "\n",
    "print(\"Description of the first tweet :\")\n",
    "print(TWEET_SCALARS_TRAIN[0])\n",
    "print(\"Shape of TWEET_SCLALAR :\", TWEET_SCALARS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for work!\n",
      "time for work!\n",
      "Labels :\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Shape of IMPORTANT_WORDS : (9, 50)\n"
     ]
    }
   ],
   "source": [
    "from descriptors.tweet_label.create_labels import create_labels\n",
    "\n",
    "# Create the decompositions of the labels\n",
    "LABEL_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "LABEL_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "LABEL_STRINGS_TRAIN = create_strings(LABEL_ORIGINALS_TRAIN, TOKENIZER)\n",
    "LABEL_STRINGS_VALID = create_strings(LABEL_ORIGINALS_VALID, TOKENIZER)\n",
    "\n",
    "# -- Get the labels -- #\n",
    "IMPORTANT_WORDS_TRAIN = create_labels(TWEET_STRINGS_TRAIN, LABEL_STRINGS_TRAIN, SENTENCE_SIZE)\n",
    "IMPORTANT_WORDS_VALID = create_labels(TWEET_STRINGS_VALID, LABEL_STRINGS_VALID, SENTENCE_SIZE)\n",
    "\n",
    "IDX = 4\n",
    "print(TWEET_ORIGINALS_TRAIN[IDX])\n",
    "print(LABEL_ORIGINALS_TRAIN[IDX])\n",
    "print(\"Labels :\")\n",
    "print(IMPORTANT_WORDS_TRAIN[IDX])\n",
    "print(\"Shape of IMPORTANT_WORDS :\", IMPORTANT_WORDS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                    weights='distance')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_NEIGHBORS = 1\n",
    "\n",
    "KNN = KNeighborsRegressor(NB_NEIGHBORS, weights=\"distance\")\n",
    "KNN.fit(TWEET_SCALARS_TRAIN, IMPORTANT_WORDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = KNN.predict(TWEET_SCALARS_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.post_processing import pred_to_string\n",
    "\n",
    "# print(type(TWEET_STRINGS_TRAIN[0][0]))\n",
    "# pred_to_string(TWEET_ORIGINALS_TRAIN[0], TWEET_STRINGS_TRAIN[0], PREDICTIONS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.post_processing import preds_to_strings\n",
    "\n",
    "RESULTS = preds_to_strings(TWEET_ORIGINALS_VALID, TWEET_STRINGS_VALID, PREDICTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee didn`t turn out good tiday but excellent!\n",
      "Coffee didn`t turn out good tiday but excellent!\n",
      "Coffee didn`\n",
      "\n",
      " ha ha surprisingly well considering!! Having a good time in the sun up in the mountains\n",
      "ha ha surprisingly well considering!! Having a good time in the sun up in the mountains\n",
      "ha ha surprisingly \n",
      "\n",
      " I love to Read  And i love Film making\n",
      "love\n",
      "I love to \n",
      "\n",
      "I need help Twitter world! Orange theraflu pills...are those going to be the drowsy or non drowsy?\n",
      "I need help Twitter world! Orange theraflu pills...are those going to be the drowsy or non drowsy?\n",
      "I need help \n",
      "\n",
      "Just like the old days  drinkin at the old spot .\n",
      "Just like the old days  drinkin at the old spot .\n",
      "Just like the \n",
      "\n",
      "Its official, I`m having the worst day. I called it a mile away\n",
      "I`m having the worst day.\n",
      "Its official, \n",
      "\n",
      "too many good shows coming up and no money to buy tickets. fmfl. brand new at the troub?\n",
      "good\n",
      "too many good \n",
      "\n",
      "My legs are so **** sore. And my feet hurt to walk. Whataday\n",
      "hurt to\n",
      "My legs are \n",
      "\n",
      " when you get your grill taken care of this morning how about a quick shot of the hands.  Don`t ask why, just do it\n",
      "when you get your grill taken care of this morning how about a quick shot of the hands.  Don`t ask why, just do it\n",
      "when you get \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(RESULTS)):\n",
    "    print(VALID_SAMPLE[i, 1])\n",
    "    print(VALID_SAMPLE[i, 2])\n",
    "    print(RESULTS[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17465140397791717\n"
     ]
    }
   ],
   "source": [
    "from utils.loss import mean_jaccard\n",
    "\n",
    "accuracy = mean_jaccard(VALID_SAMPLE[:, 2], RESULTS)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
