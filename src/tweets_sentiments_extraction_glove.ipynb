{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data base import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_data\n",
    "\n",
    "# -- Get the data -- #\n",
    "NB_SAMPLES = 10\n",
    "TRAIN_SAMPLE = Path(\"../data/samples/sample_{}_train.csv\".format(NB_SAMPLES))\n",
    "VALID_SAMPLE = Path(\"../data/samples/sample_{}_validation.csv\".format(NB_SAMPLES))\n",
    "TRAIN_SAMPLE = pd.read_csv(TRAIN_SAMPLE).to_numpy()\n",
    "VALID_SAMPLE = pd.read_csv(VALID_SAMPLE).to_numpy()\n",
    "\n",
    "\n",
    "# -- Clean the data -- #\n",
    "from utils.clean_data import clean_data\n",
    "TRAIN_SAMPLE = clean_data(TRAIN_SAMPLE)\n",
    "VALID_SAMPLE = clean_data(VALID_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20951ebfde'\n",
      "  ' While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!'\n",
      "  'While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it'\n",
      "  0]\n",
      " ['e283379cad'\n",
      "  'hates the net. ayaw bumukas ng twitter.  http://plurk.com/p/wxlxs'\n",
      "  'hates the net.' -1]\n",
      " ['0bc3d2ce9f'\n",
      "  'Im feeling so nostalgic. Im sad. But happy. I don`t now how to feel. It`s over, but not at the same time. It just feels over.  i love you.'\n",
      "  'love' 1]\n",
      " ['e316d2f54b' '_in_NH night bud' '_in_NH night bud' 0]\n",
      " ['57d554e002' 'time for work!' 'time for work!' 0]\n",
      " ['35d95a6bfa'\n",
      "  'http://twitpic.com/4wqfv - dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  'dark berry mocha frapp.. heaven.. TRY IT EVERYONE!!  here.. let me pass it to you'\n",
      "  1]\n",
      " ['a2f6c5cca9'\n",
      "  'Playing with the munchkin today, talking cakes and getting ready for a yard sale tomorrow. Not looking forward to that'\n",
      "  'Not looking forward to that' -1]\n",
      " ['dcacc9ef8f'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  'They just layed off 23 teachers in the city near me  I wonder if I should be thinking about not going into that jobfield...'\n",
      "  0]\n",
      " ['e668df7ceb' '  I hope you have a nice sleep' 'a nice' 1]]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "#### TWEET_ORIGINALS : List of the tweets : Array, shape = (len(nb_tweets))\n",
    "#### TWEET_STRINGS : List of the list of the word of each tweet : List of list of string\n",
    "#### TWEET_SCALARS : List of the description of each tweet : Array, shape = (len(nb_tweets), sentence_size * word_size)\n",
    "#### IMPORTANT_WORDS : List of the label of each tweet : Array, shape = (len(nb_tweets), sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet :\n",
      " While im stuck INSIDE in Elk Grove Village working all day   Someone should enjoy it!\n",
      "Shape of TWEET_ORIGINAL : (9,)\n"
     ]
    }
   ],
   "source": [
    "# -- Get the original tweets -- #\n",
    "TWEET_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "TWEET_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "print(\"First tweet :\")\n",
    "print(TWEET_ORIGINALS_TRAIN[0])\n",
    "print(\"Shape of TWEET_ORIGINAL :\", TWEET_ORIGINALS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition of the first tweet :\n",
      "['While', 'im', 'stuck', 'INSIDE', 'in', 'Elk', 'Grove', 'Village', 'working', 'all', 'day', 'Someone', 'should', 'enjoy', 'it', '!']\n",
      "Length of TWEET_STRING : 9\n"
     ]
    }
   ],
   "source": [
    "from descriptors.tweet_string.create_strings import create_strings\n",
    "from descriptors.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "TOKENIZER = Tokenizer()\n",
    "\n",
    "# -- Get the decomposition of the tweets -- #\n",
    "TWEET_STRINGS_TRAIN = create_strings(TWEET_ORIGINALS_TRAIN, TOKENIZER)\n",
    "TWEET_STRINGS_VALID = create_strings(TWEET_ORIGINALS_VALID, TOKENIZER)\n",
    "print(\"Decomposition of the first tweet :\")\n",
    "print(TWEET_STRINGS_TRAIN[0])\n",
    "print(\"Length of TWEET_STRING :\", len(TWEET_STRINGS_TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of the first tweet :\n",
      "[ 0.39818688 -0.70444776  0.25797363 ... -0.2136602  -0.55800931\n",
      "  0.        ]\n",
      "Shape of TWEET_SCLALAR : (9, 1001)\n"
     ]
    }
   ],
   "source": [
    "from descriptors.descriptor_glove.descriptor_glove import tweet_scalar_glove\n",
    "from utils.standardize import standardize\n",
    "\n",
    "# -- Parameters -- #\n",
    "WORD_SIZE = 50  # 50 or 100 or 200 or 300\n",
    "SENTENCE_SIZE = 20  # What ever\n",
    "FILL_WITH = 0  # If a word is not in the dictionary, [0, ..., 0] will describe it.\n",
    "SENTIMENT_WEIGHT = 2  # Multiply the sentiment by a factor\n",
    "OPTIONS = [WORD_SIZE, SENTENCE_SIZE, FILL_WITH, SENTIMENT_WEIGHT]\n",
    "\n",
    "# Get the dictionary\n",
    "PATH_DICTIONARY = Path(\"../data/glove_descriptor/glove.6B.{}d.txt\".format(WORD_SIZE))\n",
    "DICTIONARY = pd.read_csv(PATH_DICTIONARY, sep=\" \", header=None)\n",
    "\n",
    "# Additional dictionary\n",
    "ADDITIONAL_DIC = {\"..\": \"...\", \"<3\": \"love\"}\n",
    "\n",
    "# Get the sentiments\n",
    "SENTIMENTS_TRAIN = TRAIN_SAMPLE[:, -1]\n",
    "SENTIMENTS_VALID = VALID_SAMPLE[:, -1]\n",
    "\n",
    "# -- Get the decriptions of each tweets -- #\n",
    "TWEET_SCALARS_TRAIN = tweet_scalar_glove(TWEET_STRINGS_TRAIN, SENTIMENTS_TRAIN, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "TWEET_SCALARS_VALID = tweet_scalar_glove(TWEET_STRINGS_VALID, SENTIMENTS_VALID, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "\n",
    "# Standardize the tweet descriptions\n",
    "standardize(TWEET_SCALARS_TRAIN)\n",
    "standardize(TWEET_SCALARS_VALID)\n",
    "\n",
    "print(\"Description of the first tweet :\")\n",
    "print(TWEET_SCALARS_TRAIN[0])\n",
    "print(\"Shape of TWEET_SCLALAR :\", TWEET_SCALARS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for work!\n",
      "time for work!\n",
      "Labels :\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Shape of IMPORTANT_WORDS : (9, 20)\n"
     ]
    }
   ],
   "source": [
    "from descriptors.tweet_label.create_labels import create_labels\n",
    "\n",
    "# Create the decompositions of the labels\n",
    "LABEL_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "LABEL_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "LABEL_STRINGS_TRAIN = create_strings(LABEL_ORIGINALS_TRAIN, TOKENIZER)\n",
    "LABEL_STRINGS_VALID = create_strings(LABEL_ORIGINALS_VALID, TOKENIZER)\n",
    "\n",
    "# -- Get the labels -- #\n",
    "IMPORTANT_WORDS_TRAIN = create_labels(TWEET_STRINGS_TRAIN, LABEL_STRINGS_TRAIN, SENTENCE_SIZE)\n",
    "IMPORTANT_WORDS_VALID = create_labels(TWEET_STRINGS_VALID, LABEL_STRINGS_VALID, SENTENCE_SIZE)\n",
    "\n",
    "IDX = 4\n",
    "print(TWEET_ORIGINALS_TRAIN[IDX])\n",
    "print(LABEL_ORIGINALS_TRAIN[IDX])\n",
    "print(\"Labels :\")\n",
    "print(IMPORTANT_WORDS_TRAIN[IDX])\n",
    "print(\"Shape of IMPORTANT_WORDS :\", IMPORTANT_WORDS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                    weights='distance')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_NEIGHBORS = 1\n",
    "\n",
    "KNN = KNeighborsRegressor(NB_NEIGHBORS, weights=\"distance\")\n",
    "KNN.fit(TWEET_SCALARS_TRAIN, IMPORTANT_WORDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = KNN.predict(TWEET_SCALARS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-904be2b3ea1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTWEET_STRINGS_TRAIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTWEET_ORIGINALS_TRAIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTWEET_STRINGS_TRAIN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPREDICTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Dossier\\ENPC\\IMI\\MachineLearning\\Projet\\TweetSentimentExtraction\\src\\utils\\post_processing.py\u001b[0m in \u001b[0;36mpred_to_string\u001b[1;34m(original_string, original_sentence, prediction)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_character\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"$\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "from utils.post_processing import pred_to_string\n",
    "\n",
    "print(type(TWEET_STRINGS_TRAIN[0][0]))\n",
    "print(pred_to_string(TWEET_ORIGINALS_TRAIN[0], TWEET_STRINGS_TRAIN[0], PREDICTIONS[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(DATA[i, 1])\n",
    "    print(DATA[i, 2])\n",
    "    print(RESULTS[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss import jaccard\n",
    "\n",
    "AVG = 0\n",
    "for i in range(len(RESULTS)):\n",
    "    AVG += jaccard(RESULTS[i], DATA[i, 2])\n",
    "    \n",
    "    if jaccard(RESULTS[i], DATA_SAMPLE[i, 2]) != 1:\n",
    "        print(\"Error on\", i)\n",
    "AVG /= len(RESULTS)\n",
    "\n",
    "print(AVG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
