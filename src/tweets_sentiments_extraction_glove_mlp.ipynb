{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data base import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_data\n",
    "\n",
    "# -- Get the data -- #\n",
    "NB_SAMPLES = 1000\n",
    "TRAIN_SAMPLE = Path(\"../data/samples/sample_{}_train.csv\".format(NB_SAMPLES))\n",
    "VALID_SAMPLE = Path(\"../data/samples/sample_{}_validation.csv\".format(NB_SAMPLES))\n",
    "TRAIN_SAMPLE = pd.read_csv(TRAIN_SAMPLE).to_numpy()\n",
    "VALID_SAMPLE = pd.read_csv(VALID_SAMPLE).to_numpy()\n",
    "\n",
    "\n",
    "# -- Clean the data -- #\n",
    "from utils.clean_data import clean_data\n",
    "TRAIN_SAMPLE = clean_data(TRAIN_SAMPLE)\n",
    "VALID_SAMPLE = clean_data(VALID_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "#### TWEET_ORIGINALS : List of the tweets : Array, shape = (len(nb_tweets))\n",
    "#### TWEET_STRINGS : List of the list of the word of each tweet : List of list of string\n",
    "#### TWEET_SCALARS : List of the description of each tweet : Array, shape = (len(nb_tweets), sentence_size * word_size)\n",
    "#### IMPORTANT_WORDS : List of the label of each tweet : Array, shape = (len(nb_tweets), sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Parameters -- #\n",
    "WORD_SIZE = 50  # 50 or 100 or 200 or 300\n",
    "FILL_WITH = 0  # If a word is not in the dictionary, [0, ..., 0] will describe it.\n",
    "SENTIMENT_WEIGHT = 1  # Multiply the sentiment by a factor\n",
    "SENTENCE_SIZE = 50  # What ever\n",
    "OPTIONS = [WORD_SIZE, SENTENCE_SIZE, FILL_WITH, SENTIMENT_WEIGHT]\n",
    "\n",
    "\n",
    "# -- Get the original tweets -- #\n",
    "TWEET_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 1]\n",
    "TWEET_ORIGINALS_VALID = VALID_SAMPLE[:, 1]\n",
    "print(\"First tweet :\")\n",
    "print(TWEET_ORIGINALS_TRAIN[0])\n",
    "print(\"Shape of TWEET_ORIGINAL :\", TWEET_ORIGINALS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.tweet_string.create_strings import create_strings\n",
    "from descriptors.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "TOKENIZER = Tokenizer()\n",
    "\n",
    "# -- Get the decomposition of the tweets -- #\n",
    "TWEET_STRINGS_TRAIN = create_strings(TWEET_ORIGINALS_TRAIN, TOKENIZER, SENTENCE_SIZE)\n",
    "TWEET_STRINGS_VALID = create_strings(TWEET_ORIGINALS_VALID, TOKENIZER, SENTENCE_SIZE)\n",
    "print(\"Decomposition of the first tweet :\")\n",
    "print(TWEET_STRINGS_TRAIN[0])\n",
    "print(\"Length of TWEET_STRING :\", len(TWEET_STRINGS_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.descriptor_glove.descriptor_glove import tweet_scalar_glove\n",
    "from utils.standardize import standardize\n",
    "\n",
    "\n",
    "# Get the dictionary\n",
    "PATH_DICTIONARY = Path(\"../data/glove_descriptor/glove.6B.{}d.txt\".format(WORD_SIZE))\n",
    "# PATH_DICTIONARY = Path(\"../data/glove_descriptor/sample_test.txt\")\n",
    "DICTIONARY = pd.read_csv(PATH_DICTIONARY, sep=\" \", header=None)\n",
    "\n",
    "# Additional dictionary\n",
    "ADDITIONAL_DIC = {\"..\": \"...\", \"<3\": \"love\"}\n",
    "\n",
    "# Get the sentiments\n",
    "SENTIMENTS_TRAIN = TRAIN_SAMPLE[:, -1]\n",
    "SENTIMENTS_VALID = VALID_SAMPLE[:, -1]\n",
    "\n",
    "# -- Get the decriptions of each tweets -- #\n",
    "TWEET_SCALARS_TRAIN = tweet_scalar_glove(TWEET_STRINGS_TRAIN, SENTIMENTS_TRAIN, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "TWEET_SCALARS_VALID = tweet_scalar_glove(TWEET_STRINGS_VALID, SENTIMENTS_VALID, DICTIONARY, ADDITIONAL_DIC, OPTIONS)\n",
    "\n",
    "# Standardize the tweet descriptions\n",
    "standardize(TWEET_SCALARS_TRAIN)\n",
    "standardize(TWEET_SCALARS_VALID)\n",
    "\n",
    "print(\"Description of the first tweet :\")\n",
    "print(TWEET_SCALARS_TRAIN[0])\n",
    "print(\"Shape of TWEET_SCLALAR :\", TWEET_SCALARS_TRAIN.shape)\n",
    "print(TWEET_SCALARS_VALID.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descriptors.tweet_label.create_labels import create_labels\n",
    "\n",
    "# Create the decompositions of the labels\n",
    "LABEL_ORIGINALS_TRAIN = TRAIN_SAMPLE[:, 2]\n",
    "LABEL_ORIGINALS_VALID = VALID_SAMPLE[:, 2]\n",
    "LABEL_STRINGS_TRAIN = create_strings(LABEL_ORIGINALS_TRAIN, TOKENIZER, SENTENCE_SIZE)\n",
    "LABEL_STRINGS_VALID = create_strings(LABEL_ORIGINALS_VALID, TOKENIZER, SENTENCE_SIZE)\n",
    "\n",
    "# -- Get the labels -- #\n",
    "IMPORTANT_WORDS_TRAIN = create_labels(TWEET_STRINGS_TRAIN, LABEL_STRINGS_TRAIN, SENTENCE_SIZE)\n",
    "IMPORTANT_WORDS_VALID = create_labels(TWEET_STRINGS_VALID, LABEL_STRINGS_VALID, SENTENCE_SIZE)\n",
    "\n",
    "IDX = 5\n",
    "print(TWEET_ORIGINALS_TRAIN[IDX])\n",
    "print(LABEL_ORIGINALS_TRAIN[IDX])\n",
    "print(\"Labels :\")\n",
    "print(IMPORTANT_WORDS_TRAIN[IDX])\n",
    "print(\"Shape of IMPORTANT_WORDS :\", IMPORTANT_WORDS_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import binary_crossentropy\n",
    "from classifiers.classifier_mlp.classifier_mlp import ClassifierConv, ClassifierDense\n",
    "from utils.post_processing import preds_to_strings\n",
    "from utils.loss import mean_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the sentiments in the same Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training --- #\n",
    "# Parameters\n",
    "NB_EPOCHS = 2\n",
    "BATCH_SIZE = 20\n",
    "CLASS_WEIGHT = np.sum(IMPORTANT_WORDS_TRAIN, axis=0) / len(IMPORTANT_WORDS_TRAIN)\n",
    "\n",
    "# Validation data\n",
    "VALID_DATA = (TWEET_SCALARS_VALID, IMPORTANT_WORDS_VALID)\n",
    "\n",
    "# The classifier\n",
    "CLASSIFIER = ClassifierDense(WORD_SIZE, SENTENCE_SIZE)\n",
    "\n",
    "# Compile the classifier\n",
    "CLASSIFIER.compile(optimizer='adam', loss=binary_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "CLASSIFIER.fit(TWEET_SCALARS_TRAIN, IMPORTANT_WORDS_TRAIN, batch_size=BATCH_SIZE, epochs=NB_EPOCHS, \n",
    "                     validation_data=VALID_DATA, class_weight=CLASS_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing --- #\n",
    "# Parameters\n",
    "THRESHOLD_MAX = 0.01\n",
    "THRESHOLD_MIN = 0\n",
    "NB_THRESHOLDS = 30\n",
    "\n",
    "THRESHOLD_OPT = 1\n",
    "JACCARD_ACC_MAX = 0\n",
    "JACCARD_LIST = []\n",
    "\n",
    "# Predictions\n",
    "CLASSIFIER_CONV.trainable = False\n",
    "PREDICTIONS = CLASSIFIER.predict(TWEET_SCALARS_VALID)\n",
    "print(PREDICTIONS)\n",
    "\n",
    "for threshold in np.linspace(THRESHOLD_MIN, THRESHOLD_MAX, NB_THRESHOLDS):\n",
    "    # Get the predicitions with the threshold\n",
    "    PRED_THRESHOLD = PREDICTIONS > threshold\n",
    "    print(PRED_THRESHOLD[0])\n",
    "    \n",
    "    # Get the string predictions\n",
    "    PRED_STRING = preds_to_strings(TWEET_ORIGINALS_VALID, TWEET_STRINGS_VALID, PRED_THRESHOLD)\n",
    "\n",
    "    # Compute the loss\n",
    "    JACCARD_ACC = mean_jaccard(LABEL_ORIGINALS_VALID, PRED_STRING)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Jaccard score\", JACCARD_ACC)\n",
    "    print(\"Threshold\", threshold)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    # Updates\n",
    "    JACCARD_LIST.append(JACCARD_ACC)\n",
    "    if JACCARD_ACC > JACCARD_ACC_MAX:\n",
    "        JACCARD_ACC_MAX = JACCARD_ACC\n",
    "        THRESHOLD_OPT = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(JACCARD_LIST)\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"Jaccard score\")\n",
    "plt.savefig(\"../results/mlp_all.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the weights --- #\n",
    "# Path the save the weights\n",
    "PATH_SAVE_WEIGHTS = Path(\"../weights/nb_samples_{}_nb_thresholds_{}.h5\".format(NB_SAMPLES, NB_THRESHOLDS))\n",
    "\n",
    "# Save the weights\n",
    "CLASSIFIER.save_weights(str(PATH_SAVE_WEIGHTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and negative sentiment together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training --- #\n",
    "# Parameters\n",
    "NB_EPOCHS_POS_NEG = 3\n",
    "BATCH_SIZE_POS_NEG = 20\n",
    "\n",
    "# Select the positive and the negative sentiments\n",
    "TRAIN_SELECTION_POS = np.where(TWEET_SCALARS_TRAIN[:, -1] == 1)[0]\n",
    "TRAIN_SELECTION_NEG = np.where(TWEET_SCALARS_TRAIN[:, -1] == -1)[0]\n",
    "TRAIN_SELECTION_POS_NEG = np.concatenate((TRAIN_SELECTION_POS, TRAIN_SELECTION_NEG))\n",
    "# Select and withdraw the sentiment\n",
    "TWEET_SCALARS_TRAIN_SELECT = TWEET_SCALARS_TRAIN[TRAIN_SELECTION_POS_NEG][:, : -1]\n",
    "IMPORTANT_WORDS_TRAIN_POS_NEG = IMPORTANT_WORDS_TRAIN[TRAIN_SELECTION_POS_NEG]\n",
    "\n",
    "VALID_SELECTION_POS = np.where(TWEET_SCALARS_VALID[:, -1] == 1)[0]\n",
    "VALID_SELECTION_NEG = np.where(TWEET_SCALARS_VALID[:, -1] == -1)[0]\n",
    "VALID_SELECTION_POS_NEG = np.concatenate((VALID_SELECTION_POS, VALID_SELECTION_NEG))\n",
    "# Select and withdraw the sentiment\n",
    "TWEET_SCALARS_VALID_SELECT = TWEET_SCALARS_VALID[VALID_SELECTION_POS_NEG][:, : -1]\n",
    "IMPORTANT_WORDS_VALID_POS_NEG = IMPORTANT_WORDS_VALID[VALID_SELECTION_POS_NEG]\n",
    "LABEL_ORIGINALS_VALID_POS_NEG = LABEL_ORIGINALS_VALID[VALID_SELECTION_POS_NEG]\n",
    "TWEET_ORIGINALS_VALID_POS_NEG = TWEET_ORIGINALS_VALID[VALID_SELECTION_POS_NEG]\n",
    "TWEET_STRINGS_VALID_POS_NEG = np.array(TWEET_STRINGS_VALID, dtype=object)[VALID_SELECTION_POS_NEG]\n",
    "\n",
    "# Modify the data to be adapted to the convolution\n",
    "TWEET_SCALARS_TRAIN_POS_NEG = np.reshape(TWEET_SCALARS_TRAIN_SELECT, (len(TWEET_SCALARS_TRAIN_SELECT), SENTENCE_SIZE, WORD_SIZE))\n",
    "TWEET_SCALARS_VALID_POS_NEG = np.reshape(TWEET_SCALARS_VALID_SELECT, (len(TWEET_SCALARS_VALID_SELECT), SENTENCE_SIZE, WORD_SIZE))\n",
    "\n",
    "# Validation data\n",
    "VALID_DATA_POS_NEG = (TWEET_SCALARS_VALID_POS_NEG, IMPORTANT_WORDS_VALID_POS_NEG)\n",
    "\n",
    "# Set the class weight\n",
    "CLASS_WEIGHT_POS_NEG = np.sum(IMPORTANT_WORDS_TRAIN_POS_NEG, axis=0) / len(IMPORTANT_WORDS_TRAIN_POS_NEG)\n",
    "\n",
    "# The classifier\n",
    "CLASSIFIER_POS_NEG = ClassifierConv(WORD_SIZE, SENTENCE_SIZE)\n",
    "\n",
    "# Compile the classifier\n",
    "CLASSIFIER_POS_NEG.compile(optimizer='adam', loss=binary_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "CLASSIFIER_POS_NEG.fit(TWEET_SCALARS_TRAIN_POS_NEG, IMPORTANT_WORDS_TRAIN_POS_NEG, batch_size=BATCH_SIZE_POS_NEG, \n",
    "                       epochs=NB_EPOCHS_, validation_data=VALID_DATA_POS_NEG, class_weight=CLASS_WEIGHT_POS_NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing --- #\n",
    "# Parameters\n",
    "THRESHOLD_MAX_POS_NEG = 0.1\n",
    "THRESHOLD_MIN_POS_NEG = 0\n",
    "NB_THRESHOLDS_POS_NEG = 30\n",
    "\n",
    "THRESHOLD_OPT_POS_NEG = 1\n",
    "JACCARD_ACC_MAX_POS_NEG = 0\n",
    "JACCARD_LIST_POS_NEG = []\n",
    "\n",
    "# Predictions\n",
    "CLASSIFIER_POS_NEG.trainable = False\n",
    "PREDICTIONS_POS_NEG = CLASSIFIER_POS_NEG.predict(TWEET_SCALARS_VALID_POS_NEG)\n",
    "print(PREDICTIONS_POS_NEG)\n",
    "\n",
    "for threshold in np.linspace(THRESHOLD_MIN_POS_NEG, THRESHOLD_MAX_POS_NEG, NB_THRESHOLDS_POS_NEG):\n",
    "    # Get the predicitions with the threshold\n",
    "    PRED_THRESHOLD_POS_NEG = PREDICTIONS_POS_NEG > threshold\n",
    "    # print(PRED_THRESHOLD[:10])\n",
    "    \n",
    "    # Get the string predictions\n",
    "    PRED_STRING_POS_NEG = preds_to_strings(TWEET_ORIGINALS_VALID_POS_NEG, TWEET_STRINGS_VALID_POS_NEG, PRED_THRESHOLD_POS_NEG)\n",
    "\n",
    "    # Compute the loss\n",
    "    JACCARD_ACC_POS_NEG = mean_jaccard(LABEL_ORIGINALS_VALID_POS_NEG, PRED_STRING_POS_NEG)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Jaccard score\", JACCARD_ACC_POS_NEG)\n",
    "    print(\"Threshold\", threshold)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    # Updates\n",
    "    JACCARD_LIST_POS_NEG.append(JACCARD_ACC_POS_NEG)\n",
    "    if JACCARD_ACC_POS_NEG > JACCARD_ACC_MAX_POS_NEG:\n",
    "        JACCARD_ACC_MAX_POS_NEG = JACCARD_ACC_POS_NEG\n",
    "        THRESHOLD_OPT_POS_NEG = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(JACCARD_LIST_POS_NEG)\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"Jaccard score\")\n",
    "plt.savefig(\"../results/mlp_pos_neg.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the weights for positive and negative sentiments --- #\n",
    "# Path the save the weights\n",
    "PATH_SAVE_WEIGHTS_POS_NEG = Path(\"../weights/conv_nb_samples_{}_nb_thresholds_{}_pos_neg.h5\".format(NB_SAMPLES, NB_THRESHOLDS))\n",
    "\n",
    "# Save the weights\n",
    "CLASSIFIER_POS_NEG.save_weights(str(PATH_SAVE_WEIGHTS_POS_NEG))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
